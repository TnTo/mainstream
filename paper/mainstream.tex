% !TeX spellcheck = en_GB 
\documentclass[a4paper, 11pt, headings=standardclasses, tablecaptionsbelow]{scrartcl}

\usepackage[margin=2.5cm]{geometry}
\usepackage{etoolbox}
\usepackage{authblk}
\renewcommand{\Affilfont}{\small}
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}
\usepackage[style=apa, backend=biber, sorting=ynt, useprefix=true]{biblatex}
\addbibresource{mainstream.bib}
\usepackage[autostyle=false, style=english]{csquotes}
\MakeOuterQuote{"}
\usepackage[british]{babel}
\usepackage[modulo]{lineno}
\linenumbers
\usepackage{appendix}
\usepackage[hidelinks]{hyperref}
\usepackage[font=small]{caption}
\usepackage{booktabs}
\usepackage{tabularx}
\newcolumntype{Y}{>{\raggedright\arraybackslash}X}
\AtBeginEnvironment{tabularx}{\small}

\title{How is the Mainstream changed? \let\thefootnote\relax\footnotetext{This version is intended to be submitted to the XV ESHET Conference \\ An updated version of this paper and all the source code and the instructons required to replicate the paper are available at \url{https://github.com/TnTo/mainstream/}}}
\subtitle{A Topic Model insight}
\author{Michele Ciruzzi\thanks{mciruzzi@uninsubria.it - \url{https://orcid.org/0000-0003-1485-1204}}}

\begin{document}

\maketitle

%\begin{abstract}
%\end{abstract}

\section{Why studying the mainstream?}
To understand the reasons of this work and, eventually, its scientific relevance a premise is needed.

Economics as discipline presents very strong social norms and a very hierarchical structure.
As result, it is possible, and quite easy, to identify a small group of journals in which the scholars in the top US department often publish \parencite{card2013,kim2006,kim2009,dusansky1998,hamermesh2013,ellison2003,heck2006}.
Those are the same journals that are able to assure a scholar the tenure by publishing one of their papers \parencite{heckman2020}.

In other words there is a group of journals (the Top5 or the Blue Ribbon Eight) which are liked (and managed) by those who are liked (and cited and funded) by the discipline.

This concentration of power and, as consequence, prestige have, inevitably, create a notion of prestige in every aspect of the discipline: there are more prestigious theories, more prestigious research topics, more prestigious departments and so on.

But the step is short in the underfunded academia to make the post prestigious alternatives the only alternatives, inducing conformism and, as a matter of necessity, excluding less prestigious theories from scientific debate.

The prestigious economics is, politely, knows as \textit{mainstream economics} or, in a more colourful way, as \textit{orthodox economics}. The idea of an orthodoxy, and so of some competing heterodoxies, move the analysis to the semantic field of religion and faith. And it is wanted.

As many, mostly heterodox, scholars have pointed out many of the hypotheses which support the mainstream economics have to be accepted by faith, since there are not real-world evidences of their realism\footnote{Some orthodox scholars address this criticism by not considering realism as necessary in a scientific theory, but discussing this would be a too long digression.}.
The consequence is that to pursue a career in economics and be able to get a prestigious (and so well-funded) position, a profession of faith is required to every (young) scholar\footnote{An example of this kind of "religious" reasoning is proposed by Galbraith: \textit{"Accepted in reputable market orthodoxy is, as noted, the inherent perfection of the market. The market can reflect contrived or frivolous wants; it can be subject to monopoly, imperfect competition, or errors of information, but, apart from these, it is intrinsically perfect. Yet clearly the speculative episode, with increases provoking increases, is within the market itself. And so is the culminating crash. Such a thought being theologically unacceptable, it is necessary to search for external influences—in more recent times, the downturn in the summer of 1929, the budget deficit of the 1980s, and the “market mechanisms” that brought the crash of 1987. In the absence of these factors, the market presumably would have remained high and gone on up or declined gently without inflicting pain. In such fashion, the market can be held guiltless as regards inherently compelled error. There is nothing in economic life so willfully misunderstood as the great speculative episode."} \parencite{galbraith1994}}.

The last piece of the puzzle I require to justify this work is the role of economics in the society.

Economics is a social science which influences the kind of politics are realized by the governments. It provides theoretical justification, it carries out the forecast on the real-world effects, it influences the public debate providing the words to convey different ideas of the economy (and so of the society), helping to move the Overton's window.

In other words, Economics is a scientific discipline with a strong impact on the society, and the social and political implications of any economic theory must not be overlooked\footnote{The conceptual framework provided by Keynes' theories is necessary to historically and politically understand Roosevelt and the European social state of the 50s. Similarly, Margaret Thatcher's and Ronald Reagan's economic reforms would never be realized without the theoretical background provided by Friedman.}.

So, critically studying the most common economics is necessary to understand with conceptual framework the discipline is providing to the society to understand its times, to unveil which cultural and political programme it is (more or less consciously) carried out by the discipline.

Nevertheless, this work only tries to describe the mainstream economics, without arguing on the social consequences of the features highlighted. Which is still a necessary first step.

\section{The interpretative hypothesis}
I'm clearly not the first to try to describe mainstream economics and neither the first to do it quantitatively (I review some previous attempts in the next section).

Therefore, I can climb up on the shoulder of other scholars to be guided to what I can look for and which interpretative frameworks I can use.

Two qualitative observation will be the core pieces to guide this work: the \textit{empirical turn} hypothesis and the \textit{mainstream pluralism} hypothesis.

The first lens with which observe the mainstream economics is the progressive shift toward empirical research \parencite{backhouse2000,backhouse2016,backhouse2017,cherrier2022}.
Neoclassical economics, which is the theoretical framework on which mainstream economics has been based in the second half of the XX century, built its fortune on the ability to provide clear models expressed using a very formal flavour of mathematics.
In more recent times instead, in part as a reaction to the credibility crisis arose after the financial crises of the first decade of the XXI century, econometrics is becoming the standard mathematical tool of analysis and theory-driven models are being replaced by data-driven model.
This empirical turn in the scientific practices of the discipline is moving researcher away from the comprehensive economic models of the past, towards a perilous land in which the only theory is the way in which data are analysed, and every economic insight can come only from data itself.

The second lens is the mainstream pluralism hypothesis \parencite{davis2006,davis2019a,davis2019b,cedrini2018}. It has been observed that the domain of research of economics is widening and the number of different computational and mathematical tools used is increasing\footnote{This is also a consequence of the new data-driven methodologies, which are able to deal with a wider range of hypotheses than the neoclassical calculus-based approach \Parencite[see][]{cherrier2022}}. These two tendencies have allowed the emergence of many new fields in the discipline, which are very focused on a particular topic (like environmental or health economics) or on a particular method (like economic complexity or behavioural economics).
Researchers who are specialized in one of these fields rarely engage with the other niches, creating an archipelago of unconnected islands and an appearance of pluralism in economics.
The mainstream pluralism hypothesis states that this process of specialization is not developing pluralism as historically intended in economics (i.e. as a plurality of competing ontological views of the economy and the economics), but rather it is creating a plurality of loosely connected fields which shares a common ancestral set of hypothesis (the neoclassical one) and relax some of them to be able to tackle specific problems, putting themself in continuity rather than in contrast with neoclassical economics\footnote{In this sense they can view as subfields of a wider "post-neoclassical" economics, which aims to improve (or save) some parts of the neoclassical approach and legacy, rather than rebuild the discipline on completely different assumptions like the heterodoxies aim to do.}.

In the quantitative exercise that constitute the core of this paper, I try to find some evidences in favour of these two hypotheses.

\section{Distant Reading and quantitative methods in the History of Economic Thought}
A common problem in the study of the history of ideas (of which the history of economic thought is surely part) is the impossibility to accurately read a big number of textual documents in a reasonable time. A first, and often adopted, solution is to select a sample of relevant works, like the books of a single author or a handful of representative masterpieces of a period, and analyse only them.
An alternative approach is what Franco Moretti called Distant Reading \parencite{moretti2013}: instead of carefully (close) reading few works, a big number of documents are summarily analysed using statistical methods and the aid of computers.
Different features of a text can be analysed, but generally the focus is on the textual content of the document (recovered by a digital version or through OCRing) or some metadata (title, citations, authors' list).

The kind of data used and of research questions tackled, move Distant Reading close to bibliometrics and text mining studies. The fil rouge
which ties together Distant Reading studies is their motivation: overcome with quantitative techniques the impossibility to (close) read thousands of document to reconstruct the history of some idea.

The history of neoclassical economics as mainstream economics covers at least thirty years and many journals. Moreover, its influence has been systemic, it has shaped the social norms of the discipline as a whole, and sampling some "representative masterpieces" (assuming the existence of a good enough criterion to select them) would not be able to return all the shades I'm looking for.

Since those are not groundbreaking truths, this is not the first attempt to apply quantitative methods to the history of economic thought in order to get a broad picture of the evolution of the discipline in the last half-century.

In 2018, an entire special issue of the \textit{Journal of Economic Methodology} was dedicated to the topic \parencite{edwards2018a,cherrier2018a}, but the trail goes way back in the past \parencite{backhouse1997}.
The following is a brief review of some interesting works which address similar questions to mine.

\textcite{kelly2011}, using JEL codes\footnote{The \textit{Journal of Economic Literature} (JEL) codes are a widely used taxonomy system to classify economics subfields, but their reliability is discussed, since their use appears to be highly subjective \parencite{cherrier2017,kosnik2018}.}, observe a reduction of microeconomics and macroeconomics papers and an increasing of finance and development economics papers.
Moreover, the top journals over-represent microeconomics, mathematical and quantitative methods and labour economics.

\textcite{hamermesh2013} shows that the quota of empirical articles in some top journals is increasing.

\textcite{card2013} observe a bias toward theoretical microeconomics papers (and to a lesser degree applied microeconomics and macroeconomics ones) in top journals, which is reducing in the last years as empirical papers become more frequent.

\textcite{claveau2016} clusterise the citation network to highlight seven subfields which are present during the entire studied period (1956-2014): econometrics, financial economics, development and trade economics, industrial economics and game theory (the largest subfield after 2000), history of economics and macroeconomic policies, macroeconomics and monetary policies, labour economics.
In addition, they observe some clusters only at the beginning of the sequence, which are not preserved in subsequent years, and one cluster (environmental economics) which appears toward the end of the studied period.
The increase in published papers makes the recognition of clusters more difficult at the end of the series, and it is possible that some clusters went undetected in the most recent years.
Finally, they observe that it is typical for economics scholars to publish in more than one subfield.

\textcite{angrist2017} found that the share of microeconomics (mostly theoretical) papers is steadily increasing, as well as the share of development economics ones, while typical applied microeconomics fields (like labour economics or industrial organization) see their shares reducing over time.
Moreover, they describe the increase of empirical papers as a methodological shift inside the subfields, more than a change in research topics.

\textcite{ambrosino2018} explore the whole JSTOR database on a decade-per-decade basis, using a Topic Model, going deep in describing the possibilities of this kind of approach.

\textcite{fontana2019} observe an increasing usage of treatment effects models at expense of time series, a growth of development economics, game theory and education economics and a decline of industrial economics and macroeconomics. This study uses LDA on papers' full-text.

\textcite{montesinos2019} measure the most frequent words in the title of the most cited papers of the last seventy years, highlighting a shift from "theory" to "evidence" (and so empirical research).

Except for \textcite{ambrosino2018}, all the studies listed focus on a small subset of the economic journals, usually the "Top5"\footnote{\textit{The American Economic Review}, \textit{Econometrica}, \textit{Journal of Political Economy}, \textit{Quarterly Journal of Economics} and \textit{Review of Economic Studies} \parencite{heckman2020}.} or the "Blue Ribbon Eight"\footnote{The Top5 plus \textit{International Economic Review}, \textit{Journal of Economic Theory} and \textit{Review of Economics and Statistics} \parencite{dusansky1998}.}.

\section{The choice of the data}
Going back to the aim of this paper, the two question to be answered pertain the evolution of economics at least since the seventies, but, ideally, since the post-war period to be able to observe both a pre- and a post-neoclassical period.

Moreover, the focus is on the mainstream economics, which allows a strong hypothesis in choosing the data: the journals perceived by the research community as the top journals are a representative sample of what is mainstream in the community, i.e. what is considered shared knowledge, high quality research and a signal of a path to follow.

To select the top journals one can rely on the "common knowledge" represented by the Blue Ribbon Eight, and the rankings published throughout the history of the discipline or looking at some bibliometric indicator. The choice is obviously restricted by the availability of different data type.

There are three potential data types to be used: citation network, articles' metadata (particularly JEL codes) and the articles' own text (or their abstract).

I exclude the JEL codes because they are highly subjective and can change multiple times during the editorial process \parencite{kosnik2018}, and they are used to signal the (desire of) belonging to a scientific community which can be interested in reading the paper. They are, indeed, more a way to observe where the scholars desire to be, and so the relations inside the discipline, rather than the evolution of the ideas.

Similar reasoning can be done with the citation network. Since, especially in more recent times, citations have become a currency with which buys funding and (tenured) positions. According to the Goodhart's law, they have become a poor metrics to measure the relevance of a paper, since the scholars have started to play the game.

This reasoning leads me toward the use of the text of the articles. This strategy has an obvious downside (textual documents are more difficult to be analysed than a network or a taxonomy) but reduces some disadvantages of the two previous ones.
To be honest, the text of a document is still influenced by the community in which a researcher put himself (the same idea is expressed differently by scholars of different schools of thought) but it is harder to be gamed (and, sincerely, maybe it is not a real downside).
Finally, the selection bias of the authors in writing the abstracts can be avoided by using the full text of the articles, thanks to JSTOR.

\input{src/IF.tex}

This choice rises a problem: not every journal is indexed on JSTOR. Particularly, of the Blue Ribbon Eight the \textit{Journal of Economic Theory} is missing. I believe this absence is not significant, since the Top5 are still available and it (and the \textit{International Economic Review}) have slightly worse bibliometric indicators than the others and the sample is still pretty large\footnote{As shown in Table \ref{tab:IF}, the Top5 plus the \textit{Review of Economics and Statistics} are the only six generalist journals in the Top20 of each of the three different bibliometric indicators computed by SCImago (average of the 1999-2021 period).}.

The JSTOR database is particularly extended in time, going back to the early XX century. As said before, this allows to include in the study a period before the rise of neoclassical economics ad mainstream economics, the late Keynesianism of the fifties.
On the other hand, the use of the Top5 as a proxy can suffer from recentism, since their prestige has been going to consolidate in time.

Looking at some older journals' ranking \parencite[e.g.]{billings1972,hawkins1973,liebowitz1984,malouin1987,moore1972,coats1971} two other journals appears to be quite unanimously considered prestigious at the time: \textit{Economica} and \textit{The Economic Journal}. Luckily, both are available in JSTOR\footnote{There is a strong third candidate to be added to the sample: \textit{The Bell Journal of Economics}, later become \textit{The RAND Journal of Economics}. It was not included because I, the author, did not know the journal had changed its name and, so, I decided to exclude it because its publication period (as the Bell Journal of Economics) does not cover all the considered time-span. By the time I discovered the error the dataset was already consolidated. This is a brilliant example of path dependency in research and of the need of a strong domain knowledge to do quantitative studies. Nevertheless, it is the less strong candidate of the three.}.

The resulting sample is composed by: the Top5 (\textit{The American Economic Review}, \textit{Econometrica}, \textit{Journal of Political Economy}, \textit{Quarterly Journal of Economics} and \textit{Review of Economic Studies}), one additional Blue Ribbon (\textit{Review of Economics and Statistics}) and two "old glories" (\textit{Economica} and \textit{The Economic Journal}).

Finally, only the research article published between 1946 (the first year after the WWII) and 2016 (the last year for which the full-text of the articles was available for the eight journals in the sample) are considered.

\section{Strategies of analysis}
The research question of this work (if mainstream economics is going through an empirical shift and a pattern of specialization) is about the topics dealt with in mainstream economics (using a sample of top journals as proxy).
But classifying textual document based on their topics it is not an easy task. The common assumption is that if two papers use the same words, they probably are about the same topic. Even with this assumption, a set of lists of words (which is a more formal representation of a text) is still something without a clear and easy to manage mathematical abstraction.

The strong hypothesis which is commonly used in this context is to assume that the information contained in the order of the words is negligible with regard to the information conveyed by the frequency of the words. This is known as Bag-of-Words (BoW) approximation, because it is like if the words composing every document would be put in a bag and shaken until reconstructing the order become impossible. This hypothesis is used also in this study.

Using the BoW approximation the corpus can be represented as a matrix with the documents on the rows, the different words on the columns, and the frequencies as entries. The same matrix represents also a bipartite graph and a linear space of documents spanned by the words.

With a mathematical object available, it is necessary to manipulate it to obtain some answers. Two alternatives are available: declining the broad research question in a very precise one, for which is easy to measure a proxy, or using an algorithm to transform the representation of our dataset and then take some measures on the new representation.
Some results from both strategies will be presented.

Before moving into the cleaning and the analysis of the data, it remains to discuss which algorithm can be used to obtain a new, and more useful, representation of the data. A representation able to highlight semantic similarities (i.e. the topic) among documents is known in literature as Topic Model.

The common strategies in literature include inferring a latent stochastic process \parencite{blei2003,teh2005,griffiths2004,hofmann1999}, using clustering techniques on the bipartite network \parencite{gerlach2018} or on the vector space \parencite{angelov2020,grootendorst2022}, reducing the dimensionality of the matrix \parencite{kim2008}, or a combination of the previous \parencite{lancichinetti2015}\footnote{An attempt of comparison is available online at \url{https://github.com/TnTo/TopicModelBenchmark/}}.

The oldest algorithms, and among them the very popular Latent Dirichlet Allocation \parencite[LDA][]{blei2003}, require the scholars to choose a priori the number of topic to be recognized, or equivalently in how many groups divide the documents. This is clearly a strong downside of these algorithms, and, as far as I know, there is not in literature any attempt to characterize the stability of these algorithms with regard to changes in the number of topics. Moreover, most of these algorithms show little stability for different random seed and initial condition.

To overcome the limitation to not have to set a priori the number of topics two roads have been explored: improving LDA \parencite[like in][]{teh2005,lancichinetti2015} or going for a totally different paths.
Neither HDP \parencite{teh2005} nor TopicMapping \parencite{lancichinetti2015}, thou, achieve the result effectively: preliminary investigation shown that, for not well divided dataset, HDP predicts a number of topics very close to the minimum number provided by the researcher, while TopicMapping collapse in few (even one or two) very big and uninformative clusters. I cannot exclude that different choices in preprocessing the dataset, which is --as I will discuss later-- a very arbitrary process, can resolve those problems.

A different path is to use a combination of a dimensionality reduction technique, like UMAP \parencite{mcinnes2020}, followed by a clustering algorithm, like HDBSCAN \parencite{campello2013,mcinnes2017}, to classify the document. This strategy should show greater consistency across different random seed but also change the type of output obtained. LDA returns for document the probability to belong to each topic, this algorithm, instead, associate each document to at most one topic, leaving some documents unclassified. Regarding the number of topics, the researchers is able to set the minimum number of documents for each topic, allowing to resolve the model at different scales. Finally, this kind of workflow is often preceded by an embedding algorithm which provide a change-of-basis function from the word-spanned vector space to an abstract one, determined with machine learning techniques \parencite{angelov2020,grootendorst2022}.
The presence of unclassified points, and a personal preference to not use embedding techniques, has put me on a different path.

The strategy used in this work relies on the hierarchical stochastic block model algorithm \parencite[hSBM][]{peixoto2019,gerlach2018}, which aims to clusterize each partition of the bipartite graph, at different scale. It yields a collection of models, each one a zoomed version of the previous, in which each word is assigned to a topic and each document is assigned to a group. For those used to LDA this is an important difference: LDA infers a single latent layer (i.e. Words---Topic---Documents), while hSBM infers two of them (Words---Topics---Groups---Documents). Moreover, hSBM provides both a hard-clustering version (like UMAP-HDBSCAN in which each document or word belongs to only one cluster) and a soft-clustering version (like LDA in which the membership is probabilistic). The problem of this algorithm is it computational cost which is prohibitive for the soft-clustering version and still elevated for the hard-clustering one\footnote{The final version of the dataset requires more than 10GB of RAM to be processed and more than 10 hours on my personal laptop. One cause of this is the single-threaded nature of the algorithm. A previous version of the dataset with a smaller number of words removed required more than 20GB of RAM. The dataset with the documents' metadata is a 300MB SQLite file.}.

The final choice is anyway hSBM, mostly because it is the algorithm that requires the researcher to input the smallest number of parameters, it allows zooming into each group to analyse its composition (i.e. of which subgroups is composed) and it provides an unsupervised method to choose among the results of different initial conditions.

\section{Corpus cleaning}
The starting point of this analysis\footnote{The steps of the analysis can be followed and reproduced using the files \texttt{main.py} and \texttt{mainstream.py} at \url{https://github.com/TnTo/mainstream/}.} is a dataset downloaded from Constellate, a web-based software to analyse JSTOR papers focused on text-mining, which include all the available documents published in the eight available Blue Ribbon Journals, \textit{Economica}, the \textit{Economic Journal} and the \textit{Bell Journal of Economics}\footnote{The Constellate IDs of the dataset downloaded are: \texttt{31bcd322-032f-2ba5-37c4-c2ca96952ebf} (7 Blue Ribbons - 1900-1954), \texttt{1a0cd895-717b-dcc2-940d-a4d7ed069aea} (7 Blue Ribbons - 1955-1984), \texttt{03aa84ad-ec5e-290c-69e7-721535714c9e} (7Blue Ribbons - 1985-2022) (Downloaded 19/09/2022), \texttt{4788f182-8ec3-8eb8-ac5b-a1b318b03dbb} (Economica and Economic Journal), \texttt{c55cf30b-737b-9a83-3325-8d82a71bd8e6} (Bell Journal of Economics) (Downloaded 08/10/2022). These datasets can be retrieved online at \url{https://constellate.org/dataset/} followed by the dataset ID.}.

\input{src/dataset.tex}

First, the dataset is converted from the original jsonl format to a SQLite database. The resulting dataset is the "Original dataset" in table \ref*{tab:dataset}.

Then the dataset goes through a first cleaning step. The articles published in one of the eight journals of the sample between 1946 and 2016 are selected. Of these, only those with at least an author listed (this heuristic discards mostly editorial articles, notes, comments and similar non-research-articles), indexed on JSTOR, classified as "research-article" and with the list of the words in the articles with their frequencies, are kept. In addition, non-alphabetical characters are removed from each word listed, which is then lowercased and stemmed (with the NLTK Snowball stemmer). Then words composed of one or two characters are discarded.
The result of this step is the second row of the table \ref{tab:dataset}.
The 55\% of the original documents are discarded in this way and the number of unique words is reduced by 80\%.

This intermediate version of the corpus discards all the obvious non-inclusion from the dataset. It discards over-downloaded articles (selecting by journal and year), wrong type of contents (notes, indexes, editorials...) and a first bunch of non-informative words. Particularly, it modifies OCR errors removing number and typographic characters and very short words which are either non-informative on their own (I, a, of, on, it, ...) or too generic after stemming.

The only non-obvious choice in this step is if to stem or not to stem. It is safe to assume that the semantic difference among, for example, stutter, stutters and stutterer it is not relevant in describing the broad topic covered. In this way we are able to remove some noise from the input of the algorithm, selecting (using some previous knowledge) only the relevant part of the information. A second argument is more practical: stemming reduces a lot the dimension of the documents-words matrix, allowing the topic model algorithm to run faster with less resources.
Finally, stemming is not the only way to achieve similar results, but similar results can be obtained also using lemmatization or embedding.

The second cleaning step is more arbitrary.
First, only the words (that at this point have already been lowercased and stemmed) which appears in at least 35 documents (approximately 1 every 1~000) and at most in 9000 documents (approximately one fourth) are kept. This lower and upper bounds are determined by the desired type of results, in other words how many groups are expected. The answer is ten to twenty, because less than ten would be too broad and archetypal (macroeconomics, microeconomics, econometrics and few others) to capture any change, while more than twenty can be too detailed to be easily distinguished and interpreted. Following this reasoning, a word which compare in over a quarter characterize a topic too broad to be of interest (if not the discipline as whole), while a word comparing in less than a document every one thousand brings a shade of meaning too specific to be of interest at the scale of interest (or more probably is an OCR error). Than a small set of \LaTeX command\footnote{\texttt{landscap}, \texttt{normalfont}, \texttt{usepackageamsbsi}, \texttt{renewcommandsfdefaultwncyss}, \texttt{declaremaths}, \texttt{usepackagestmaryrd}, \texttt{usepackagetextcomp}, \texttt{renewcommandrmdefaultwncyr}, \texttt{newcommandcyr}, \texttt{usepackageototfontenc}, \texttt{pagestyleempti}, \texttt{renewcommandencodingdefaultot}, \texttt{declaretextfontcommandtextcyrcyr}, \texttt{documentclassaastex}, \texttt{usepackageportlandxspac}, \texttt{usepackagepifont}, \texttt{usepackageamssymb}, \texttt{selectfont}, \texttt{usepackageamsmathamsxtra}, \texttt{usepackagemathrsf}, \texttt{usepackagebm}, \texttt{usepackageamsfont}, \texttt{begindocu}, \texttt{enddocu}, \texttt{iin}, \texttt{frac}.} are removed from the list of words, because they signal not a topic but a way of typesetting and digitalize (and so, probably, a journal).
Finally, only the documents with at least 1~500 tokens (i.e. total words) after the first cleaning step are kept. Those are document longer than three or four pages, and it is another heuristic to remove errata corriges, short notes and similar documents.

At the end the final dataset, on which the topic model is inferred, is composed by slightly more than thirty thousands documents (93\% of the intermediate dataset), thirty thousands unique words (2\%) and thirty-eight millions tokens (21\%).

% plots of documents distribution

\section{Model selection, validation and description}

\section{Interpreting the model}

\subsection{The empirical turn}

\subsection{Mainstream pluralism?}

\section{Limitations}
\subsection{Choosing the corpus}
% Data availability

\subsection{The quest for a good Topic Modelling algorithm}

\subsection{Interpreting the topics}
% Mixing sources

\subsection{Assess the robustness}
% Preprocessing
% Clustering
% Interpretation

\section{What's next?}
\subsection{Rephrasing the questions for a theory-driven Distant Reading}

\subsection{Delve into the hierarchy}

\subsection{Enhancing the database}
% Citations, JEL, Missing journals

\section*{Acknowledgments}
This paper is a (very) revised version of my master thesis, which benefitted from the excellent guidance of my supervisors, Michele Caselle, Mario Cedrini e Alessandra Durio, and the useful critiques provided by Matteo Osella.
This work would not have been possible without the (computational) help provided by Diletta Abbonato in the exploratory phase, the, often ignored, advices from Carlo Debernardi, and the freedom to pursue secondary projects during the PhD granted me by Eugenio Caverzasi (and his support).
The revisions of the various versions of this manuscript have taken advantage of Alice Re's scrupulousness and helpfulness.
Finally, I want to thank the whole DR2 community for the numerous stimuli in continuing working on quantitative history of the ideas and particularly Paolo Babbiotti, and Eugenio Petrovich, who, in a cold January in Rome, taught me how to write an Acknowledgments section.

\clearpage
\begin{refcontext}[sorting=nyt]
  \printbibliography
\end{refcontext}
\clearpage

\begin{appendices}
  \section{Results from different random seeds}

\end{appendices}

\end{document}